'use client'
import React, { useState, useRef, useEffect } from 'react'

import { Box, Button, MenuItem, Select, Typography, CircularProgress, Stack, styled } from '@mui/material'

// üåü –ê–Ω–∏–º–∞—Ü–∏—è –ø—É–ª—å—Å–∞—Ü–∏–∏ —Å —É—á–µ—Ç–æ–º –≥—Ä–æ–º–∫–æ—Å—Ç–∏
const RecordingIndicator = styled('div')<{ volume: number }>(({ theme, volume }) => ({
  width: 100,
  height: 100,
  borderRadius: '50%',
  backgroundColor: theme.palette.primary.main,
  position: 'relative',
  display: 'flex',
  alignItems: 'center',
  justifyContent: 'center',
  transform: `scale(${1 + volume * 0.5})`, // –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –º–∞—Å—à—Ç–∞–± –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –≥—Ä–æ–º–∫–æ—Å—Ç–∏
  opacity: 0.8 + volume * 0.2, // –ò–∑–º–µ–Ω—è–µ–º –ø—Ä–æ–∑—Ä–∞—á–Ω–æ—Å—Ç—å –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –≥—Ä–æ–º–∫–æ—Å—Ç–∏
  transition: 'transform 0.1s ease, opacity 0.1s ease' // –ü–ª–∞–≤–Ω–∞—è –∞–Ω–∏–º–∞—Ü–∏—è
}))

const languages = [
  { code: 'en', label: '–ê–Ω–≥–ª–∏–π—Å–∫–∏–π' },
  { code: 'ru', label: '–†—É—Å—Å–∫–∏–π' },
  { code: 'es', label: '–ò—Å–ø–∞–Ω—Å–∫–∏–π' }
]

export default function Speech() {
  const [recording, setRecording] = useState(false)
  const [translatedText, setTranslatedText] = useState('')
  const [translatedAudioUrl, setTranslatedAudioUrl] = useState('')
  const [loading, setLoading] = useState(false)
  const [selectedLang, setSelectedLang] = useState('en')
  const [volume, setVolume] = useState(0) // –°–æ—Å—Ç–æ—è–Ω–∏–µ –¥–ª—è –≥—Ä–æ–º–∫–æ—Å—Ç–∏

  const mediaRecorderRef = useRef<MediaRecorder | undefined>(undefined)
  const audioChunks = useRef<Blob[]>([])
  const audioContextRef = useRef<AudioContext>()
  const analyserRef = useRef<AnalyserNode | undefined>(undefined)
  const animationFrameRef = useRef<number | undefined>(undefined)

  const startRecording = async () => {
    try {
      setRecording(true)
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true })

      // –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Web Audio API –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –≥—Ä–æ–º–∫–æ—Å—Ç–∏
      audioContextRef.current = new AudioContext()
      const source = audioContextRef.current.createMediaStreamSource(stream)

      analyserRef.current = audioContextRef.current.createAnalyser()
      analyserRef.current.fftSize = 256
      source.connect(analyserRef.current)

      // –ù–∞—Å—Ç—Ä–æ–π–∫–∞ MediaRecorder –¥–ª—è –∑–∞–ø–∏—Å–∏
      mediaRecorderRef.current = new MediaRecorder(stream, { mimeType: 'audio/webm' })
      audioChunks.current = []

      mediaRecorderRef.current.ondataavailable = e => {
        if (e.data.size > 0) audioChunks.current.push(e.data)
      }

      mediaRecorderRef.current.start()

      // –ó–∞–ø—É—Å–∫ –∞–Ω–∞–ª–∏–∑–∞ –≥—Ä–æ–º–∫–æ—Å—Ç–∏
      const updateVolume = () => {
        if (!analyserRef.current) return

        const dataArray = new Uint8Array(analyserRef.current.fftSize)

        analyserRef.current.getByteTimeDomainData(dataArray)
        let sum = 0

        for (let i = 0; i < dataArray.length; i++) {
          const a = dataArray[i] / 128 - 1

          sum += a * a
        }

        const rms = Math.sqrt(sum / dataArray.length)

        setVolume(Math.min(rms * 10, 1)) // –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –≥—Ä–æ–º–∫–æ—Å—Ç–∏ (0-1)
        animationFrameRef.current = requestAnimationFrame(updateVolume)
      }

      updateVolume()
    } catch (error) {
      console.error('–û—à–∏–±–∫–∞ –ø—Ä–∏ –Ω–∞—á–∞–ª–µ –∑–∞–ø–∏—Å–∏:', error)
      setRecording(false)
    }
  }

  const stopRecording = () => {
    if (mediaRecorderRef.current && recording) {
      mediaRecorderRef.current.stop()
      setRecording(false)
      mediaRecorderRef.current.onstop = handleSendAudio

      // –û—Å—Ç–∞–Ω–æ–≤–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –≥—Ä–æ–º–∫–æ—Å—Ç–∏
      if (animationFrameRef.current) {
        cancelAnimationFrame(animationFrameRef.current)
      }

      if (audioContextRef.current) {
        audioContextRef.current.close()
      }

      setVolume(0) // –°–±—Ä–∞—Å—ã–≤–∞–µ–º –≥—Ä–æ–º–∫–æ—Å—Ç—å
    }
  }

  async function convertWebMToWav(webmBlob: Blob) {
    const audioContext = new AudioContext({ sampleRate: 44100 })
    const arrayBuffer = await webmBlob.arrayBuffer()
    const audioBuffer = await audioContext.decodeAudioData(arrayBuffer)

    const numOfChannels = audioBuffer.numberOfChannels
    const sampleRate = audioBuffer.sampleRate
    const numOfFrames = audioBuffer.length

    const wavBuffer = new ArrayBuffer(44 + numOfFrames * numOfChannels * 2)
    const view = new DataView(wavBuffer)

    function writeString(view: DataView<ArrayBuffer>, offset: number, string: string) {
      for (let i = 0; i < string.length; i++) {
        view.setUint8(offset + i, string.charCodeAt(i))
      }
    }

    function write32(view: DataView<ArrayBuffer>, offset: number, value: number) {
      view.setUint32(offset, value, true)
    }

    function write16(view: DataView<ArrayBuffer>, offset: number, value: number) {
      view.setUint16(offset, value, true)
    }

    writeString(view, 0, 'RIFF')
    write32(view, 4, 36 + numOfFrames * numOfChannels * 2)
    writeString(view, 8, 'WAVE')
    writeString(view, 12, 'fmt ')
    write32(view, 16, 16)
    write16(view, 20, 1)
    write16(view, 22, numOfChannels)
    write32(view, 24, sampleRate)
    write32(view, 28, sampleRate * numOfChannels * 2)
    write16(view, 32, numOfChannels * 2)
    write16(view, 34, 16)
    writeString(view, 36, 'data')
    write32(view, 40, numOfFrames * numOfChannels * 2)

    let offset = 44

    for (let i = 0; i < numOfFrames; i++) {
      for (let channel = 0; channel < numOfChannels; channel++) {
        let sample = audioBuffer.getChannelData(channel)[i] * 0x7fff

        sample = Math.max(-0x8000, Math.min(0x7fff, sample))
        view.setInt16(offset, sample, true)
        offset += 2
      }
    }

    return new Blob([view], { type: 'audio/wav' })
  }

  const handleSendAudio = async () => {
    const audioBlob = new Blob(audioChunks.current, { type: 'audio/webm' })

    try {
      setLoading(true)
      const wavBlob = await convertWebMToWav(audioBlob)
      const formData = new FormData()

      formData.append('file', wavBlob, 'recording.wav')

      const response = await fetch('http://localhost:8000/process-audio/', {
        method: 'POST',
        body: formData
      })

      if (!response.ok) {
        throw new Error('–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ç–ø—Ä–∞–≤–∫–µ –∞—É–¥–∏–æ')
      }

      const resultBlob = await response.blob()
      const audioUrlResponse = URL.createObjectURL(resultBlob)

      setTranslatedAudioUrl(audioUrlResponse)

      // –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ—Ç—Å—è, —á—Ç–æ —Å–µ—Ä–≤–µ—Ä –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–µ–∫—Å—Ç –æ—Ç–¥–µ–ª—å–Ω–æ
      const resultText = await response.text() // –∏–ª–∏ JSON, –µ—Å–ª–∏ —Å–µ—Ä–≤–µ—Ä –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç JSON

      setTranslatedText(resultText)
    } catch (error) {
      console.error('–û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ –∞—É–¥–∏–æ:', error)
    } finally {
      setLoading(false)
    }
  }

  useEffect(() => {
    return () => {
      // –û—á–∏—Å—Ç–∫–∞ –ø—Ä–∏ —Ä–∞–∑–º–æ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞
      if (animationFrameRef.current) {
        cancelAnimationFrame(animationFrameRef.current)
      }

      if (audioContextRef.current) {
        audioContextRef.current.close()
      }
    }
  }, [])

  return (
    <Box p={2}>
      <Stack spacing={3} alignItems='center'>
        <Select value={selectedLang} onChange={e => setSelectedLang(e.target.value)} fullWidth>
          {languages.map(lang => (
            <MenuItem key={lang.code} value={lang.code}>
              {lang.label}
            </MenuItem>
          ))}
        </Select>

        {/* üåü –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∑–∞–ø–∏—Å—å—é */}
        <Box sx={{ display: 'flex', flexDirection: 'column', alignItems: 'center', gap: 2 }}>
          {!recording ? (
            <Button
              variant='contained'
              color='primary'
              size='large'
              onClick={startRecording}
              sx={{ width: 120, height: 120, borderRadius: '50%' }}
            >
              üé§ –ù–∞—á–∞—Ç—å
            </Button>
          ) : (
            <>
              <RecordingIndicator volume={volume}>
                <Typography variant='caption' color='white'>
                  –ó–∞–ø–∏—Å—å
                </Typography>
              </RecordingIndicator>
              <Button
                variant='contained'
                color='secondary'
                size='large'
                onClick={stopRecording}
                sx={{ width: 120, height: 60, borderRadius: 30 }}
              >
                –°—Ç–æ–ø
              </Button>
            </>
          )}
        </Box>

        <Typography variant='caption' color='text.secondary'>
          {recording ? '–ò–¥—ë—Ç –∑–∞–ø–∏—Å—å...' : '–ù–∞–∂–º–∏ –¥–ª—è –∑–∞–ø–∏—Å–∏'}
        </Typography>

        {loading && <CircularProgress />}

        {translatedText && (
          <Typography variant='body1'>
            –ü–µ—Ä–µ–≤–æ–¥: <strong>{translatedText}</strong>
          </Typography>
        )}

        {translatedAudioUrl && <audio controls src={translatedAudioUrl} />}
      </Stack>
    </Box>
  )
}
